{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1)HOUSE PRICE  PREDICTION**"
      ],
      "metadata": {
        "id": "TpKef2TnJTgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit scikit-learn pandas numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEw16SpgKfLO",
        "outputId": "6fddfa10-9174-4a71-a1ed-881db6a6aece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.49.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.0)\n",
            "Downloading streamlit-1.49.1-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.49.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"HousingData.csv\")\n",
        "print(data.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UDRGQ_vLbLt",
        "outputId": "02c43d9a-9e61-4233-99c9-94b19faa12e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
            "       'PTRATIO', 'B', 'LSTAT', 'MEDV'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVNlg0e2m5JR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "7b471c1e-c745-439c-897c-11dc5943a359"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained ‚úÖ | MSE: 25.02\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://36cf75130e9fe0d5eb.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://36cf75130e9fe0d5eb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# ---------------------------\n",
        "# Load Dataset\n",
        "# ---------------------------\n",
        "data = pd.read_csv(\"HousingData.csv\")   # make sure file is uploaded or in same folder\n",
        "\n",
        "# Features and Target\n",
        "X = data.drop(\"MEDV\", axis=1)\n",
        "y = data[\"MEDV\"]\n",
        "\n",
        "# Handle missing data\n",
        "X = X.fillna(X.mean())\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Model trained ‚úÖ | MSE: {mse:.2f}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Gradio Interface\n",
        "# ---------------------------\n",
        "\n",
        "# Function for prediction\n",
        "def predict_house_price(*features):\n",
        "    # Convert inputs to DataFrame\n",
        "    input_data = pd.DataFrame([features], columns=X.columns)\n",
        "    # Scale input\n",
        "    input_scaled = scaler.transform(input_data)\n",
        "    # Predict\n",
        "    prediction = model.predict(input_scaled)[0]\n",
        "    return f\"Predicted Price: ${prediction:.2f} (in $1000s)\"\n",
        "\n",
        "# Create input fields dynamically for each feature\n",
        "inputs = []\n",
        "for feature in X.columns:\n",
        "    min_val = float(X[feature].min())\n",
        "    max_val = float(X[feature].max())\n",
        "    mean_val = float(X[feature].mean())\n",
        "    inputs.append(gr.Number(label=feature, value=mean_val))\n",
        "\n",
        "# Gradio Interface\n",
        "demo = gr.Interface(\n",
        "    fn=predict_house_price,\n",
        "    inputs=inputs,\n",
        "    outputs=\"text\",\n",
        "    title=\"üè† House Price Prediction\",\n",
        "    description=f\"Predicts median house price (MEDV). Model MSE = {mse:.2f}\"\n",
        ")\n",
        "\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xy282z9IPMYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "# ---------------------------\n",
        "# Load Dataset\n",
        "# ---------------------------\n",
        "data = pd.read_csv(\"HousingData.csv\")   # make sure the file is in the same folder\n",
        "\n",
        "# Features and Target\n",
        "X = data.drop(\"MEDV\", axis=1)\n",
        "y = data[\"MEDV\"]\n",
        "\n",
        "# Handle missing data\n",
        "X = X.fillna(X.mean())\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# ---------------------------\n",
        "# Regression Metrics\n",
        "# ---------------------------\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\n--- Regression Metrics ---\")\n",
        "print(f\"MSE  : {mse:.2f}\")\n",
        "print(f\"MAE  : {mae:.2f}\")\n",
        "print(f\"R¬≤   : {r2:.2f}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Convert Regression ‚Üí Classification\n",
        "# ---------------------------\n",
        "# Define bins (cover full range of y, including negatives if any)\n",
        "bins = [y.min() - 1, 20, 35, np.inf]   # ensure all values fall into bins\n",
        "labels = [\"Low\", \"Medium\", \"High\"]\n",
        "\n",
        "# Convert continuous target & predictions to categories\n",
        "y_test_class = pd.cut(y_test, bins=bins, labels=labels)\n",
        "y_pred_class = pd.cut(y_pred, bins=bins, labels=labels)\n",
        "\n",
        "# Drop NaN rows (if any values fall outside bins)\n",
        "mask = ~y_test_class.isna() & ~y_pred_class.isna()\n",
        "y_test_class = y_test_class[mask]\n",
        "y_pred_class = y_pred_class[mask]\n",
        "\n",
        "# Encode labels to numeric\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(labels)\n",
        "\n",
        "y_test_encoded = encoder.transform(y_test_class)\n",
        "y_pred_encoded = encoder.transform(y_pred_class)\n",
        "\n",
        "# Classification Metrics\n",
        "print(f\"\\n--- Classification Metrics ---\")\n",
        "print(\"Accuracy :\", accuracy_score(y_test_encoded, y_pred_encoded))\n",
        "print(\"Precision:\", precision_score(y_test_encoded, y_pred_encoded, average=\"weighted\"))\n",
        "print(\"Recall   :\", recall_score(y_test_encoded, y_pred_encoded, average=\"weighted\"))\n",
        "print(\"F1-score :\", f1_score(y_test_encoded, y_pred_encoded, average=\"weighted\"))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test_encoded, y_pred_encoded))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_encoded, y_pred_encoded, target_names=labels))\n",
        "\n",
        "# ---------------------------\n",
        "# Gradio Interface\n",
        "# ---------------------------\n",
        "def predict_house_price(*features):\n",
        "    input_data = pd.DataFrame([features], columns=X.columns)\n",
        "    input_scaled = scaler.transform(input_data)\n",
        "    prediction = model.predict(input_scaled)[0]\n",
        "\n",
        "    # Convert to category\n",
        "    category = pd.cut([prediction], bins=bins, labels=labels)[0]\n",
        "\n",
        "    return f\"Predicted Price: ${prediction:.2f} (in $1000s)\\nCategory: {category}\"\n",
        "\n",
        "# Create input fields dynamically\n",
        "inputs = []\n",
        "for feature in X.columns:\n",
        "    mean_val = float(X[feature].mean())\n",
        "    inputs.append(gr.Number(label=feature, value=mean_val))\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=predict_house_price,\n",
        "    inputs=inputs,\n",
        "    outputs=\"text\",\n",
        "    title=\"üè† House Price Prediction\",\n",
        "    description=f\"Regression MSE = {mse:.2f}, R¬≤ = {r2:.2f}\\nClassification used for Low/Medium/High categories.\"\n",
        ")\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "kWxHwIwJPMVt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0ef37ae1-d42b-4ed3-8004-3be97e91b5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Regression Metrics ---\n",
            "MSE  : 25.02\n",
            "MAE  : 3.15\n",
            "R¬≤   : 0.66\n",
            "\n",
            "--- Classification Metrics ---\n",
            "Accuracy : 0.83\n",
            "Precision: 0.8375026833631486\n",
            "Recall   : 0.83\n",
            "F1-score : 0.8292292587137947\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 4  0  3]\n",
            " [ 0 39  9]\n",
            " [ 1  4 40]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         Low       0.80      0.57      0.67         7\n",
            "      Medium       0.91      0.81      0.86        48\n",
            "        High       0.77      0.89      0.82        45\n",
            "\n",
            "    accuracy                           0.83       100\n",
            "   macro avg       0.83      0.76      0.78       100\n",
            "weighted avg       0.84      0.83      0.83       100\n",
            "\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://258a17684f084119b0.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://258a17684f084119b0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SPAM MAIL DETECTION**"
      ],
      "metadata": {
        "id": "_Lf-qvfONhES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "# ---------------------------\n",
        "# Load Dataset\n",
        "# ---------------------------\n",
        "data = pd.read_csv(\"spam_ham_dataset.csv\")  # replace with your dataset filename\n",
        "\n",
        "# Features and Target\n",
        "X = data[\"text\"]\n",
        "y = data[\"label\"]   # or use 'label_num' if already numeric\n",
        "\n",
        "# Encode labels to numeric if needed\n",
        "encoder = LabelEncoder()\n",
        "y_encoded = encoder.fit_transform(y)\n",
        "\n",
        "# ---------------------------\n",
        "# Text Preprocessing using TF-IDF\n",
        "# ---------------------------\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=3000)  # limit to top 3000 words\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# ---------------------------\n",
        "# Train-test split\n",
        "# ---------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, y_encoded, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# Train Naive Bayes Model\n",
        "# ---------------------------\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# ---------------------------\n",
        "# Classification Metrics\n",
        "# ---------------------------\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(f\"\\n--- Classification Metrics ---\")\n",
        "print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average=\"weighted\"))\n",
        "print(\"Recall   :\", recall_score(y_test, y_pred, average=\"weighted\"))\n",
        "print(\"F1-score :\", f1_score(y_test, y_pred, average=\"weighted\"))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=encoder.classes_))\n",
        "\n",
        "# ---------------------------\n",
        "# Gradio Interface\n",
        "# ---------------------------\n",
        "def predict_spam(message):\n",
        "    # Transform the input message using the same TF-IDF vectorizer\n",
        "    msg_tfidf = vectorizer.transform([message])\n",
        "    prediction = model.predict(msg_tfidf)[0]\n",
        "    label = encoder.inverse_transform([prediction])[0]\n",
        "    return f\"This message is classified as: {label.upper()}\"\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=predict_spam,\n",
        "    inputs=gr.Textbox(label=\"Enter email message\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"üìß Spam Mail Detection using Naive Bayes\",\n",
        "    description=\"Enter a message to classify as HAM (not spam) or SPAM.\\nModel uses TF-IDF + MultinomialNB.\"\n",
        ")\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "Il_fwqYgPMb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7d20ea08-ae8d-4b9f-dcfe-f08e758c94ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Classification Metrics ---\n",
            "Accuracy : 0.9449275362318841\n",
            "Precision: 0.9464348470925827\n",
            "Recall   : 0.9449275362318841\n",
            "F1-score : 0.9453847504718782\n",
            "\n",
            "Confusion Matrix:\n",
            " [[705  37]\n",
            " [ 20 273]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.97      0.95      0.96       742\n",
            "        spam       0.88      0.93      0.91       293\n",
            "\n",
            "    accuracy                           0.94      1035\n",
            "   macro avg       0.93      0.94      0.93      1035\n",
            "weighted avg       0.95      0.94      0.95      1035\n",
            "\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://343a3bc0ba32e2ac7e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://343a3bc0ba32e2ac7e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}